package dev.langchain4j.model

import dev.langchain4j.model.output.Response

/**
 * Represents a handler for streaming responses from a language model.
 * The handler is invoked each time the model generates a new token in a textual response.
 * If the model executes a tool instead, [.onComplete] will be invoked instead.
 *
 * @param <T> The type of the response.
</T> */
interface StreamingResponseHandler<T> {
    /**
     * Invoked each time the language model generates a new token in a textual response.
     * If the model executes a tool instead, this method will not be invoked; [.onComplete] will be invoked instead.
     *
     * @param token The newly generated token, which is a part of the complete response.
     */
    fun onNext(token: String?)

    /**
     * Invoked when the language model has finished streaming a response.
     * If the model executes one or multiple tools, it is accessible via [dev.langchain4j.data.message.AiMessage.toolExecutionRequests].
     *
     * @param response The complete response generated by the language model.
     * For textual responses, it contains all tokens from [.onNext] concatenated.
     */
    fun onComplete(response: Response<T>?) {
    }

    /**
     * This method is invoked when an error occurs during streaming.
     *
     * @param error The error that occurred
     */
    fun onError(error: Throwable?)
}
