package dev.langchain4j.model.chat.response

import dev.langchain4j.data.message.AiMessage
import dev.langchain4j.model.chat.StreamingChatModel

/**
 * Represents a handler for a [StreamingChatModel] response.
 *
 * @see StreamingChatModel
 */
interface StreamingChatResponseHandler {
    /**
     * Invoked each time the model generates a partial response (usually a single token) in a textual response.
     * If the model decides to execute a tool instead, this method will not be invoked;
     * [.onCompleteResponse] will be invoked instead.
     *
     * @param partialResponse The partial response (usually a single token), which is a part of the complete response.
     */
    fun onPartialResponse(partialResponse: String?)

    /**
     * Invoked when the model has finished streaming a response.
     * If the model requests the execution of one or multiple tools,
     * this can be accessed via [ChatResponse.aiMessage] -> [AiMessage.toolExecutionRequests].
     *
     * @param completeResponse The complete response generated by the model.
     * For textual responses, it contains all tokens from [.onPartialResponse] concatenated.
     */
    fun onCompleteResponse(completeResponse: ChatResponse)

    /**
     * This method is invoked when an error occurs during streaming.
     *
     * @param error The error that occurred
     */
    fun onError(error: Throwable)
}
